Answer Key for Quiz Set 1
========================================

MCQ 1 (Original Q4): Option A - L1 Regularization (Lasso)
MCQ 2 (Original Q5): Option A - The model focuses on minimizing training error and may overfit
MCQ 3 (Original Q7): Option D - The probability that the classifier ranks a random positive instance higher than a random negative instance
MCQ 4 (Original Q8): Option B - 16
MCQ 5 (Original Q3): Option B - Gini Impurity
MCQ 6 (Original Q6): Option A - It splits data into k parts, trains on k-1 parts, tests on 1 part, repeats k times
MCQ 7 (Original Q1): Option B - Reducing bias typically increases variance, and vice versa
MCQ 8 (Original Q2): Option D - 0.857
Subjective 1 (Original Q1): a) Left: underfitting (too simple), Middle: good fit (captures pattern without noise), Right: overfitting (memorizes noise), b) High λ to increase bias and reduce variance
Subjective 2 (Original Q6): Linear: Linear relationship, normality | Interpretable, fast | Limited to linear patterns. Trees: No assumptions | Interpretable, handles non-linear | Prone to overfitting. k-NN: Locality assumption | Simple, non-parametric | Computationally expensive, curse of dimensionality
Subjective 3 (Original Q8): a) Bootstrap sampling + feature randomness reduces variance. b) AdaBoost: sequential, focuses on mistakes; Bagging: parallel, reduces variance. c) RF: noisy data, parallel processing; AdaBoost: clean data, need high accuracy
Subjective 4 (Original Q4): a) High bias (both curves plateau at low values), b) More complex model, feature engineering, c) Gap = 3%
Subjective 5 (Original Q7): ∂J/∂θ = (1/m) Σ (h_θ(x⁽ⁱ⁾) - y⁽ⁱ⁾) x⁽ⁱ⁾. Update rule: θ := θ - α(1/m) Σ (h_θ(x⁽ⁱ⁾) - y⁽ⁱ⁾) x⁽ⁱ⁾
Subjective 6 (Original Q6): a) Support vectors define the decision boundary and margin; only they matter for classification, b) 15/200 = 7.5%, c) Linear: O(d), Kernel: O(number of SVs)
Subjective 7 (Original Q7): a) MSE = 0.0175, b) Loss = MSE + 0.05 * Σ(wi²)
Subjective 8 (Original Q8): a) Depth = 2, b) Gini = 1 - (0.5² + 0.375² + 0.125²) = 0.609, c) Reduce overfitting and improve generalization